
from typing import Tuple
import matplotlib.pyplot as plt



X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)
X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)

def split_data(X: np.array, Y: np.array, percent_test: int=70) -> tuple[np.array, np.array]:
	assert(X.shape[1]==Y.shape[1]), 		f"Y labels dont match X data"
		
	



y_pre_good=np.array([[0.000001, 0.00001, 0.9999999]]).T

y_pre_bed=np.array([[0.1,0.1,0.8]]).T
y=np.array([[0,0,1]]).T 
cross_entropy_loss(y,y_pre_good)




l1 = Dense_Layer(Tanh, 784, 784, batch_size=3)
l2 = Dense_Layer(Tanh, 784, 89, batch_size=3)
l3 = Dense_Layer(Tanh, 89, 89, batch_size=3)
l4 = Dense_Layer(Tanh, 89, 16, batch_size=3)
model = Neural_Network([l1, l2, l3, l4], C=Classes, batch_size=32)
x=np.array((data_set["numpy_image"]))
model.forward(x)


a=np.random.randn(4,3) # a.shape = (4, 3)a.shape=(4,3)

b = np.random.randn(1, 3) # b.shape = (1, 3)b.shape=(1,3)

c = a*b
x=(np.array(data_set["numpy_image"]))
x.reshape((784,len(x)))


l = Dense_Layer(Tanh, 3, 5, batch_size=3)
x = np.atleast_2d(np.array([[1,2,3,5,9],[2,3,6,7,2],[5,8,9,2,1]])).T
l.forward(x)
print(l.B)
e = np.atleast_2d(np.array([0.35,0.6,0.05])).T
l.backward(e)
l.forward(x)



X = np.array([[1,2,3],[4,5,6]])
print(X)
X = np.atleast_2d(np.mean(X, axis=1)).T
print(X)
#E = np.atleast_2d(np.array([1,2,3])).T
#print(Tanh.derive(E))




X, Y = load_data_set("EMNIST_DATA_SET/", 32, Classes, equilibrium=False)
Y[:3]


np.atleast_2d

arr = np.random.randint(100, size=(10, 10))



for i in softmax(np.array([0,2,3,7])).astype(np.float):
	print(i)



plt.plot(np.linspace(-10,10,200),softmax(np.linspace(-10,10,200)))
plt.plot(np.linspace(-10,10,200), derivative(softmax,domain=(np.linspace(-10,10,200))))


plt.plot(np.linspace(-10,10,200),ReLu(np.linspace(-10,10,200)))
plt.plot(np.linspace(-10,10,200), derivative(ReLu,domain=(np.linspace(-10,10,200))))



l = data_set["numpy_image"][2]


lay1 = Dense_Layer(28,28,ReLu)
f1 = lay1.forward(l, exp_standard = True)
#f1[abs(f1) > 1]
plt.plot(list(range(len(f1.flatten()))),f1.flatten())




# looks promising

_5x5filter = np.array([
	[-1, 1, 0, 0, 0],
	[-1, 1, 0, 0, 0],
	[-1, 1, 0, 0, 0],
	[-1, 1, 0, 0, 0],
	[-1, 1, 0, 0, 0]
])

filter_edge = np.array([
	[-1,-1,-1],
	[-1,8,-1],
	[-1,-1,-1]
])

filter_line_left = np.array([
	[-1,1,0],
	[-1,1,0],
	[-1,1,0]
])

filter_line_down = np.rot90(filter_line_left)
filter_line_right = np.rot90(filter_line_down)
filter_line_up = np.rot90(filter_line_right)

filter_line_horizontal = np.array([
	[-1,-1,-1],
	[2,2,2],
	[-1,-1,-1]
])

filter_line_vertical = np.rot90(filter_line_horizontal)


filter_line_slanted_up = np.array([
	[-1,-1,2],
	[-1,2,-1],
	[2,-1,-1]
])

filter_line_slanted_down = np.rot90(filter_line_slanted_up)

print(filter_line_down,"\n", filter_line_left,"\n", filter_line_up,"\n", filter_line_right)
#op = convolution(pool(convolution(l,filter_line_slanted_up)),filter_line_horizontal )
#op += op.mean()
op = pool(op)
op = pool(op)
op =convolution(l,filter_line_horizontal)
op = ReLu(op)
op = convolution(op, filter_line_slanted_up)
op = ReLu(op)

#op = pool(op)
#op = convolution(l, filter_line_slanted_up)
#op = pool(op)
#op = convolution(l, filter_line_slanted_up)
op *= 255.
#op = (op - op.min())/(op.max() - op.min()) * 255.0
display(Image.fromarray(op).convert("L"))
#convolution(l,cf)



Y = chain_derivative(functions = [lambda u: np.exp(u), lambda u: np.log(u), lambda u: u+np.sqrt(1+u**2)])
X = np.linspace(-10,10,200)
#X, Y = list(X), list(Y)

plt.plot(X,Y)
o = (X + np.sqrt(1+X**2))/np.sqrt(1+X**2)
plt.plot(X, o, color="r")
#plt.plot(list(x),list(y))
#len(list(y))


data_set = load_data_set("./EMNIST_DATA_SET/", batch=16*3)
data_set

tst = np.ones((34,34))
tst = convolution(tst, filter_line_horizontal)
tst = convolution(tst, filter_line_horizontal)
tst = pool(tst)
tst = convolution(tst, filter_line_horizontal)
tst = convolution(tst, filter_line_horizontal)
tst = convolution(tst, filter_line_horizontal)
tst = convolution(tst, filter_line_horizontal)
tst



im1 = load_numpy_image("EMNIST_DATA_SET/A/A_char_100097.jpeg")
im2 = load_numpy_image("EMNIST_DATA_SET/A/A_char_100236.jpeg")
#im3 = load_numpy_image("docs/illustrations/backpropagation_humour.PNG")
im3 = load_numpy_image("EMNIST_DATA_SET/A/A_char_100236.jpeg")
im4 = load_numpy_image("EMNIST_DATA_SET/A/A_char_102241.jpeg")
imgs= [im1,im2,im3, im4]
make_input_matrix(imgs).shape
lst=(make_random_batch("EMNIST_DATA_SET/", 10, Classes, False))
lst


data_set = make_random_batch("EMNIST_DATA_SET/", 16, Classes, equilibrium=False)
#print(data_set)
#make_labels(data_set)
make_labels_matrix(data_set, Classes)
#np.zeros((16, 5))

	
	
l1 = Dense_Layer(ReLu, 4, 6, batch_size=3)
l1.W

class Dense_Layer:
	
	def __init__(self, A: Function, F: int, N: int, batch_size: int=32):
		self.A = A #fonction d'activation
		self.N = N #Neurones dans cette couche
		self.F = F #neurones dans la couche precedente, features
		self.S = batch_size #S: nombre d'echantillons, samples
    #self.W = np.atleast_2d(np.random.randn(self.N, self.F)*np.sqrt(2/self.F)) #He Initialisation
                    
        #self.W = np.atleast_2d(np.random.randn(self.N, self.F)*np.sqrt(2/self.F)) #He Initialisation
		#self.B = np.atleast_2d(np.zeros((self.N,self.S))) #biais nuls
		self.B = np.zeros((self.N,1))
	def forward(self, X: np.ndarray):
		X = np.atleast_2d(X)
		assert(X.shape[0]==self.F and self.S==X.shape[1]),			f"W of dim {self.W.shape} and X of dim {X.shape} can't be multiplied"
		self.X = X
		self.Z = np.dot(self.W,self.X)+self.B 
		self.A_of_z = self.A.output(self.Z)
		return self.A_of_z
	def backward(self, A_next: np.ndarray, lr: float=1.0):
		A_next = np.atleast_2d(A_next)
		assert(A_next.shape[0]==self.N and 1==E.shape[1]),			f"E of shape {E.shape} is not a proper error vector, try transpose"


		# self.dAZ_dz = self.A.derive(self.Z)
		# self.W -= lr*np.dot(E*self.dAZ_dz, self.X.T)
		# nB = self.B[:,0] - np.atleast_2d(np.mean(self.dAZ_dz, axis=1)).T
		# self.B -= nB
		# output_grad = np.dot(E*self.dAZ_dz, self.W.T)
		

		dZ1 = np.dot(self.W.T, A_NEXT)*self.A.derive(self.A_of_z)
		dW1 = (1./m)*np.dot(dZ1, self.X.T)
		db1 = (1./m)*np.sum(dZ1, axis=1, keepdims=True)
		return dZ1

#	dZ2 = A2 - Y
#	dW2 = (1./self.S)*np.dot(dZ2, A1.T)
#	db2 = (1./m)*np.sum(dZ2, axis=1, keepdims=True)
